#%% md
# Preparation
#%%
"""
# Download model
huggingface-cli download tiiuae/Falcon3-3B-Instruct-1.58bit --local-dir ~/models/tiiuae/Falcon3-3B-Instruct-1.58bit
# Compile
python setup_env.py -md ~/models/tiiuae/Falcon3-3B-Instruct-1.58bit -q i2_s
"""
#%% md
# Importing Libraries
#%%
import os
import gc
from dataclasses import dataclass
import torch
from transformers import AutoModelForCausalLM
from utils import set_seed
from speculative_decoding import BitNet
#from evaluate import LlmEvaluator
#%% md
# Configuration
#%%
@dataclass
class CONFIG:
    # Debug
    debug: bool = False
    verbose: bool = True

    # Model
    ## Tokenizer
    tokenizer_id: str = "tiiuae/Falcon3-3B-Instruct"
    ## HuggingFace
    model_path: str       = "/home/pathfinder/models/tiiuae/Falcon3-3B-Instruct"  # 3B
    model_small_path: str = "/home/pathfinder/models/tiiuae/Falcon3-1B-Instruct"  # 1B
    ## GGUF (1bit)
    bitnet_path: str = "/home/pathfinder/models/tiiuae/Falcon3-3B-Instruct-1.58bit/ggml-model-i2_s.gguf" # 3B

    ctx_size: int = 1024

    # Generation
    max_new_tokens: int = 100
    ## Speculative Decoding
    num_assistant_tokens: int = 5
    confidence_threshold: float = 0.25

    # Device
    n_threads: int = 12

    # Seed
    seed = 42

config = CONFIG()
#%%
os.environ["TOKENIZERS_PARALLELISM"] = "false"
#%%
set_seed(config.seed)
#%%
system_prompt = "You are an helpful assistant."
user_prompt = "Given a rational number, write it as a fraction in lowest terms and calculate the product of the resulting numerator and denominator. For how many rational numbers between 0 and 1 will $20_{}^{}!$ be the resulting product?"
#%% md
# Model
#%%
bitnet = BitNet()
bitnet.start_server(
    bitnet_path=config.bitnet_path,
    ctx_size=config.ctx_size,
    n_threads=config.n_threads,
    verbose=config.verbose
)
bitnet.init_tokenizer(
    tokenizer_id=config.tokenizer_id,
    verbose=False
)
bitnet.init_model(
    model_path=config.model_path,
    verbose=True
)
#%%
text = bitnet.format_falcon_prompt(
    system_prompt=system_prompt,
    user_prompt=user_prompt
)
print(text)
#%% md
# Generation
#%%
# 3B (bf16)
bitnet.generate_hf(
    text=text,
    max_new_tokens=config.max_new_tokens,
    stream=True,
    verbose=config.verbose
)
#%%
# 3B (1bit)
bitnet.generate_gguf(
    text=text,
    max_new_tokens=config.max_new_tokens,
    verbose=config.verbose
)
#%%
gc.collect()
torch.cuda.empty_cache()
#%% md
## Speculative Decoding
#%%
# 32bit-1bit
bitnet.speculative_decoding(
    text=text,
    max_new_tokens=config.max_new_tokens,
    num_assistant_tokens=config.num_assistant_tokens,
    confidence_threshold=config.confidence_threshold,
    verbose=True
)
gc.collect()
torch.cuda.empty_cache()
#%%
# 32bit-4bit

#%%
# 3B-1B
bitnet.speculative_decoding_hf(
    small_model=AutoModelForCausalLM.from_pretrained(
        config.model_small_path,
        device_map="cpu",
        dtype=torch.float32
    ),
    text=text,
    max_new_tokens=config.max_new_tokens,
    num_assistant_tokens=config.num_assistant_tokens,
    confidence_threshold=config.confidence_threshold,
    verbose=True
)
gc.collect()
torch.cuda.empty_cache()
#%% md
# Evaluation
#%% md
## HellaSwag
# %%
from datasets import load_dataset
from tqdm.notebook import tqdm
import numpy as np
import time

# Load HellaSwag dataset
try:
    hellaswag_dataset = load_dataset("hellaswag", "default", split="validation")
    print("HellaSwag dataset loaded successfully.")
except Exception as e:
    print(f"Failed to load HellaSwag dataset: {e}")
    # Fallback to a small dummy dataset for demonstration if loading fails
    dummy_data = {
        'ctx': ["The man is sitting on a bench.", "A woman is walking her dog."],
        'endings': [["He is reading a book.", "He is eating a sandwich.", "He is sleeping.", "He is playing a guitar."],
                    ["The dog is barking.", "The dog is chasing a cat.", "The dog is a golden retriever.", "The dog is on a leash."]],
        'label': ['0', '3'],
        'activity_label': ['reading', 'walking'],
        'source_id': ['dummy1', 'dummy2']
    }
    from datasets import Dataset
    hellaswag_dataset = Dataset.from_dict(dummy_data)
    print("Using a dummy dataset for demonstration.")


# Evaluation function
def evaluate_hellaswag(model_instance, generation_fn, dataset, num_samples=100, verbose=False):
    correct_predictions = 0
    total_time = 0
    total_tokens = 0

    # Limit the number of samples for quick evaluation
    dataset = dataset.select(range(min(num_samples, len(dataset))))

    for sample in tqdm(dataset, desc=f"Evaluating with {generation_fn.__name__}"):
        context = sample['ctx']
        endings = sample['endings']
        correct_label = int(sample['label'])

        prompt = model_instance.format_falcon_prompt(
            system_prompt=system_prompt,
            user_prompt=context
        )
        
        log_likelihoods = []
        
        # To measure performance, we generate from the prompt and check if it matches.
        # A more robust method is to calculate perplexity for each ending.
        # Here, we'll use a simplified generation-based approach for speed comparison.
        
        start_time = time.time()
        # We generate a small number of tokens and see which ending it starts with.
        # This is a heuristic and not a standard HellaSwag evaluation.
        # The main goal is to compare generation speed.
        generated_text = generation_fn(
            text=prompt,
            max_new_tokens=20, # Generate a few tokens to see the direction
            verbose=False # Keep notebook clean
        ).strip()
        end_time = time.time()

        total_time += (end_time - start_time)
        total_tokens += len(model_instance.tokenizer.encode(generated_text))

        # Check which ending is the most likely prefix of the generated text
        best_match = -1
        max_len = -1
        for i, ending in enumerate(endings):
            if generated_text.startswith(ending.strip()):
                # Simple prefix matching
                if len(ending) > max_len:
                    best_match = i
                    max_len = len(ending)

        if best_match == correct_label:
            correct_predictions += 1

    accuracy = (correct_predictions / len(dataset)) * 100
    avg_time_per_sample = total_time / len(dataset)
    tokens_per_second = total_tokens / total_time if total_time > 0 else 0
    
    print(f"\n--- Results for {generation_fn.__name__} ---")
    print(f"Accuracy: {accuracy:.2f}%")
    print(f"Average time per sample: {avg_time_per_sample:.4f} seconds")
    print(f"Tokens per second: {tokens_per_second:.2f} tokens/s")
    
    return {
        "accuracy": accuracy,
        "avg_time": avg_time_per_sample,
        "tokens_per_sec": tokens_per_second
    }

# --- Run Evaluation ---
num_eval_samples = 100 # Adjust as needed

# 1. Standard Hugging Face Generation
hf_results = evaluate_hellaswag(
    bitnet,
    lambda text, max_new_tokens, verbose: bitnet.generate_hf(text=text, max_new_tokens=max_new_tokens, stream=False, verbose=verbose),
    hellaswag_dataset,
    num_samples=num_eval_samples
)

gc.collect()
torch.cuda.empty_cache()

# 2. Speculative Decoding (BitNet GGUF as draft)
speculative_results = evaluate_hellaswag(
    bitnet,
    lambda text, max_new_tokens, verbose: bitnet.speculative_decoding(
        text=text,
        max_new_tokens=max_new_tokens,
        num_assistant_tokens=config.num_assistant_tokens,
        confidence_threshold=config.confidence_threshold,
        verbose=verbose
    ),
    hellaswag_dataset,
    num_samples=num_eval_samples
)

gc.collect()
torch.cuda.empty_cache()

# --- Comparison ---
print("\n\n--- HellaSwag Evaluation Summary ---")
print(f"Evaluated on {num_eval_samples} samples.\n")
print(f"{'Method':<30} | {'Accuracy (%)':<15} | {'Tokens/sec':<15}")
print("-" * 65)
print(f"{'Standard HF':<30} | {hf_results['accuracy']:<15.2f} | {hf_results['tokens_per_sec']:<15.2f}")
print(f"{'Speculative Decoding':<30} | {speculative_results['accuracy']:<15.2f} | {speculative_results['tokens_per_sec']:<15.2f}")

speedup = speculative_results['tokens_per_sec'] / hf_results['tokens_per_sec'] if hf_results['tokens_per_sec'] > 0 else float('inf')
print(f"\nSpeculative decoding provided a ~{speedup:.2f}x speedup in tokens/second.")
