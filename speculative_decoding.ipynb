{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Preparation",
   "id": "f6de6af755fdb886"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-17T04:11:42.686413Z",
     "start_time": "2025-10-17T04:11:42.676268Z"
    }
   },
   "source": [
    "\"\"\"\n",
    "# Download model\n",
    "huggingface-cli download tiiuae/Falcon3-1B-Instruct-1.58bit --local-dir ~/models/tiiuae/Falcon3-1B-Instruct-1.58bit\n",
    "# Compile\n",
    "python setup_env.py -md ~/models/tiiuae/Falcon3-1B-Instruct-1.58bit -q i2_s\n",
    "\n",
    "#.build/bin/llama-server -m /home/pathfinder/models/tiiuae/Falcon3-1B-Instruct-1.58bit/ggml-model-i2_s.gguf --host 127.0.0.1 --port 8080\n",
    "\n",
    "# Download Falcon3-1B-Instruct\n",
    "huggingface-cli download tiiuae/Falcon3-1B-Instruct --local-dir ~/models/tiiuae/Falcon3-1B-Instruct\n",
    "\"\"\""
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Download model\\nhuggingface-cli download tiiuae/Falcon3-1B-Instruct-1.58bit --local-dir ~/models/tiiuae/Falcon3-1B-Instruct-1.58bit\\n# Compile\\npython setup_env.py -md ~/models/tiiuae/Falcon3-1B-Instruct-1.58bit -q i2_s\\n\\n#.build/bin/llama-server -m /home/pathfinder/models/tiiuae/Falcon3-1B-Instruct-1.58bit/ggml-model-i2_s.gguf --host 127.0.0.1 --port 8080\\n\\n# Download Falcon3-1B-Instruct\\nhuggingface-cli download tiiuae/Falcon3-1B-Instruct --local-dir ~/models/tiiuae/Falcon3-1B-Instruct\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Importing Libraries",
   "id": "dffe6bf4c285bea9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T04:11:47.269128Z",
     "start_time": "2025-10-17T04:11:42.690658Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from utils import set_seed\n",
    "from speculative_decoding import BitNet\n",
    "#from evaluate import LlmEvaluator"
   ],
   "id": "62ddb7478aa2932d",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Configuration",
   "id": "f8e21d5ab2dcbde7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T04:11:47.319701Z",
     "start_time": "2025-10-17T04:11:47.315421Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@dataclass\n",
    "class CONFIG:\n",
    "    # Debug\n",
    "    debug: bool = False\n",
    "    verbose: bool = True\n",
    "\n",
    "    # Model\n",
    "    ## Tokenizer\n",
    "    tokenizer_id: str = \"tiiuae/Falcon3-1B-Instruct\"\n",
    "    ## HuggingFace\n",
    "    model_path: str = \"/home/pathfinder/models/tiiuae/Falcon3-1B-Instruct\"  # 1B\n",
    "    four_bit_path: str = \"/home/pathfinder/models/tiiuae/Falcon3-3B-Instruct-GPTQ-Int4\" # 1B (4bit)\n",
    "    model_large_path: str = \"/home/pathfinder/models/tiiuae/Falcon3-3B-Instruct\" # 3B\n",
    "    ## GGUF (1bit)\n",
    "    bitnet_path: str = \"/home/pathfinder/models/tiiuae/Falcon3-1B-Instruct-1.58bit/ggml-model-i2_s.gguf\"\n",
    "\n",
    "    ctx_size: int = 1024\n",
    "\n",
    "    # Generation\n",
    "    max_new_tokens: int = 25\n",
    "    ## Speculative Decoding\n",
    "    num_assistant_tokens: int = 5\n",
    "    assistant_confidence_threshold: float = 0.4\n",
    "\n",
    "    # Device\n",
    "    n_threads: int = 12\n",
    "\n",
    "    # Seed\n",
    "    seed = 42\n",
    "\n",
    "config = CONFIG()"
   ],
   "id": "a6b66f6771b15607",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T04:11:47.326376Z",
     "start_time": "2025-10-17T04:11:47.323430Z"
    }
   },
   "cell_type": "code",
   "source": "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"",
   "id": "784cb1797748b6fc",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T04:11:47.338442Z",
     "start_time": "2025-10-17T04:11:47.330731Z"
    }
   },
   "cell_type": "code",
   "source": "set_seed(config.seed)",
   "id": "ea880def9f4f766c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to 42\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T04:11:47.349909Z",
     "start_time": "2025-10-17T04:11:47.347016Z"
    }
   },
   "cell_type": "code",
   "source": [
    "system_prompt = \"You are an helpful assistant.\"\n",
    "user_prompt = \"Explain quantum mechanics in detail please.\"\n",
    "assistant_response = \"Quantum mechanics is a fundamental theory in physics that describes the physical properties of nature at the scale of atoms and subatomic particles, such as electrons, protons, and photons.\""
   ],
   "id": "16b44cddf2e377f0",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model",
   "id": "896a55c2e60df54a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T04:11:54.002906Z",
     "start_time": "2025-10-17T04:11:47.353350Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bitnet = BitNet()\n",
    "bitnet.start_server(\n",
    "    bitnet_path=config.bitnet_path,\n",
    "    ctx_size=config.ctx_size,\n",
    "    n_threads=config.n_threads,\n",
    "    verbose=config.verbose\n",
    ")\n",
    "bitnet.init_tokenizer(\n",
    "    tokenizer_id=config.tokenizer_id,\n",
    "    verbose=False\n",
    ")\n",
    "bitnet.init_model(\n",
    "    model_path=config.model_path,\n",
    "    verbose=True\n",
    ")"
   ],
   "id": "ab56cfc0e8ed0735",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting llama-server on 127.0.0.1:8080\n",
      "✅ Server is ready.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['output_attentions']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(131072, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-17): 18 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-06)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((2048,), eps=1e-06)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=131072, bias=False)\n",
      ")\n",
      "Number of parameters: 1.67B\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T04:11:54.035688Z",
     "start_time": "2025-10-17T04:11:54.021299Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = bitnet.format_falcon_prompt(\n",
    "    system_prompt=system_prompt,\n",
    "    user_prompt=user_prompt,\n",
    "    assistant_response=assistant_response\n",
    ")\n",
    "print(text)"
   ],
   "id": "ff3e4928039e3dca",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are an helpful assistant.\n",
      "<|user|>\n",
      "Explain quantum mechanics in detail please.\n",
      "<|assistant|>\n",
      "Quantum mechanics is a fundamental theory in physics that describes the physical properties of nature at the scale of atoms and subatomic particles, such as electrons, protons, and photons.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T04:12:00.797405Z",
     "start_time": "2025-10-17T04:11:54.040095Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 32bit-2bit\n",
    "bitnet.generate_hf(\n",
    "    assistant_model=AutoModelForCausalLM.from_pretrained(\n",
    "        config.four_bit_path,\n",
    "        device_map=\"cuda\",\n",
    "        output_attentions=True,\n",
    "    ),\n",
    "    text=text,\n",
    "    max_new_tokens=10,\n",
    "    num_assistant_tokens=5,\n",
    "    assistant_confidence_threshold=0.25,\n",
    "    stream=True,\n",
    "    verbose=True\n",
    ")"
   ],
   "id": "335dd1d204a4eb64",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pathfinder/miniconda3/envs/bitnet-cpp/lib/python3.9/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:411: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(ctx, input, qweight, scales, qzeros, g_idx, bits, maxq):\n",
      "/home/pathfinder/miniconda3/envs/bitnet-cpp/lib/python3.9/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:419: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n",
      "/home/pathfinder/miniconda3/envs/bitnet-cpp/lib/python3.9/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float16)\n",
      "CUDA extension not installed.\n",
      "CUDA extension not installed.\n",
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are an helpful assistant.\n",
      "<|user|>\n",
      "Explain quantum mechanics in detail please.\n",
      "<|assistant|>\n",
      "Quantum mechanics is a fundamental theory in physics that describes the physical properties of nature at the scale of atoms and subatomic particles, such as electrons, protons, and photons. It is a complex and counterintuitive theory that has been\n",
      "\n",
      "--- outputs 객체 디버깅 정보 ---\n",
      "outputs 타입: <class 'transformers.generation.utils.GenerateDecoderOnlyOutput'>\n",
      "outputs 속성: ['__annotations__', '__class__', '__class_getitem__', '__contains__', '__dataclass_fields__', '__dataclass_params__', '__delattr__', '__delitem__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__ior__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__or__', '__post_init__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__ror__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', 'attentions', 'clear', 'copy', 'fromkeys', 'get', 'hidden_states', 'items', 'keys', 'logits', 'move_to_end', 'past_key_values', 'pop', 'popitem', 'scores', 'sequences', 'setdefault', 'to_tuple', 'update', 'values']\n",
      "outputs.sequences.shape: torch.Size([1, 74])\n",
      "outputs.scores 길이: 10\n",
      "outputs.scores[0] 타입: <class 'torch.Tensor'>\n",
      "outputs.scores[0] shape (첫 번째 스코어 텐서): torch.Size([1, 131072])\n",
      "--- outputs 객체 디버깅 정보 끝 ---\n",
      "\n",
      "\n",
      "\u001B[95m──────────────────────────────────────────────────\n",
      "🧠 Generation Info (Hugging Face)\n",
      "──────────────────────────────────────────────────\u001B[0m\n",
      "\u001B[94m💬 User Input:\u001B[0m\n",
      "<|system|>\n",
      "You are an helpful assistant.\n",
      "<|user|>\n",
      "Explain quantum mechanics in detail please.\n",
      "<|assistant|>\n",
      "Quantum mechanics is a fundamental theory in physics that describes the physical properties of nature at the scale of atoms and subatomic particles, such as electrons, protons, and photons.\n",
      "\n",
      "\u001B[92m🟢 Generated Text:\u001B[0m\n",
      " It is a complex and counterintuitive theory that has been\n",
      "\n",
      "\u001B[94m📊 Timings:\u001B[0m\n",
      "├─ Total Time: 3.11s\n",
      "└─ Decode: 311.33 ms/token, 3.21 tokens/s\n",
      "\u001B[94m📦 Tokens:\u001B[0m\n",
      "├─ Prefilled: 64\n",
      "└─ Decoded: 10\n",
      "\u001B[94m✨ Speculative Decoding:\u001B[0m\n",
      "\n",
      "\u001B[95m──────────────────────────────────────────────────\n",
      "💡 Token Probabilities\n",
      "──────────────────────────────────────────────────\u001B[0m\n",
      "| Step | Token           |  Probability |\n",
      "|------|-----------------|--------------|\n",
      "|    1 |  It             |       63.56% |\n",
      "|    2 |  is             |       25.70% |\n",
      "|    3 |  a              |       48.73% |\n",
      "|    4 |  complex        |       19.01% |\n",
      "|    5 |  and            |       48.46% |\n",
      "|    6 |  counterintuiti |       20.87% |\n",
      "|    7 |  theory         |       52.42% |\n",
      "|    8 |  that           |       50.88% |\n",
      "|    9 |  has            |       25.30% |\n",
      "|   10 |  been           |       44.32% |\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' It is a complex and counterintuitive theory that has been'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Generation",
   "id": "ceb2d26bf8121fc8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T04:12:01.399676Z",
     "start_time": "2025-10-17T04:12:00.811398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1B (bf16)\n",
    "bitnet.generate_hf(\n",
    "    text=text,\n",
    "    max_new_tokens=config.max_new_tokens,\n",
    "    stream=True,\n",
    "    verbose=config.verbose\n",
    ")"
   ],
   "id": "ee07d4eb922e80b6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are an helpful assistant.\n",
      "<|user|>\n",
      "Explain quantum mechanics in detail please.\n",
      "<|assistant|>\n",
      "Quantum mechanics is a fundamental theory in physics that describes the physical properties of nature at the scale of atoms and subatomic particles, such as electrons, protons, and photons. It is a complex and counterintuitive theory that has been developed over the past two centuries, and it has revolutionized our understanding of the\n",
      "\n",
      "--- outputs 객체 디버깅 정보 ---\n",
      "outputs 타입: <class 'transformers.generation.utils.GenerateDecoderOnlyOutput'>\n",
      "outputs 속성: ['__annotations__', '__class__', '__class_getitem__', '__contains__', '__dataclass_fields__', '__dataclass_params__', '__delattr__', '__delitem__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__ior__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__or__', '__post_init__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__ror__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', 'attentions', 'clear', 'copy', 'fromkeys', 'get', 'hidden_states', 'items', 'keys', 'logits', 'move_to_end', 'past_key_values', 'pop', 'popitem', 'scores', 'sequences', 'setdefault', 'to_tuple', 'update', 'values']\n",
      "outputs.sequences.shape: torch.Size([1, 89])\n",
      "outputs.scores 길이: 25\n",
      "outputs.scores[0] 타입: <class 'torch.Tensor'>\n",
      "outputs.scores[0] shape (첫 번째 스코어 텐서): torch.Size([1, 131072])\n",
      "--- outputs 객체 디버깅 정보 끝 ---\n",
      "\n",
      "\n",
      "\u001B[95m──────────────────────────────────────────────────\n",
      "🧠 Generation Info (Hugging Face)\n",
      "──────────────────────────────────────────────────\u001B[0m\n",
      "\u001B[94m💬 User Input:\u001B[0m\n",
      "<|system|>\n",
      "You are an helpful assistant.\n",
      "<|user|>\n",
      "Explain quantum mechanics in detail please.\n",
      "<|assistant|>\n",
      "Quantum mechanics is a fundamental theory in physics that describes the physical properties of nature at the scale of atoms and subatomic particles, such as electrons, protons, and photons.\n",
      "\n",
      "\u001B[92m🟢 Generated Text:\u001B[0m\n",
      " It is a complex and counterintuitive theory that has been developed over the past two centuries, and it has revolutionized our understanding of the\n",
      "\n",
      "\u001B[94m📊 Timings:\u001B[0m\n",
      "├─ Total Time: 0.58s\n",
      "└─ Decode: 23.14 ms/token, 43.22 tokens/s\n",
      "\u001B[94m📦 Tokens:\u001B[0m\n",
      "├─ Prefilled: 64\n",
      "└─ Decoded: 25\n",
      "\n",
      "\u001B[95m──────────────────────────────────────────────────\n",
      "💡 Token Probabilities\n",
      "──────────────────────────────────────────────────\u001B[0m\n",
      "| Step | Token           |  Probability |\n",
      "|------|-----------------|--------------|\n",
      "|    1 |  It             |       63.56% |\n",
      "|    2 |  is             |       25.70% |\n",
      "|    3 |  a              |       48.73% |\n",
      "|    4 |  complex        |       19.01% |\n",
      "|    5 |  and            |       48.46% |\n",
      "|    6 |  counterintuiti |       20.87% |\n",
      "|    7 |  theory         |       52.42% |\n",
      "|    8 |  that           |       50.88% |\n",
      "|    9 |  has            |       25.30% |\n",
      "|   10 |  been           |       44.32% |\n",
      "|   11 |  developed      |       16.19% |\n",
      "|   12 |  over           |       65.83% |\n",
      "|   13 |  the            |       35.12% |\n",
      "|   14 |  past           |       43.78% |\n",
      "|   15 |  two            |       48.85% |\n",
      "|   16 |  centuries      |       99.78% |\n",
      "|   17 | ,               |       40.36% |\n",
      "|   18 |  and            |       25.14% |\n",
      "|   19 |  it             |       82.04% |\n",
      "|   20 |  has            |       23.85% |\n",
      "|   21 |  revolutionized |       49.34% |\n",
      "|   22 |  our            |       91.07% |\n",
      "|   23 |  understanding  |       99.97% |\n",
      "|   24 |  of             |       99.96% |\n",
      "|   25 |  the            |       92.00% |\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' It is a complex and counterintuitive theory that has been developed over the past two centuries, and it has revolutionized our understanding of the'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T04:12:02.238683Z",
     "start_time": "2025-10-17T04:12:01.404716Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1B (1bit)\n",
    "bitnet.generate_gguf(\n",
    "    text=text,\n",
    "    max_new_tokens=config.max_new_tokens,\n",
    "    verbose=config.verbose\n",
    ")"
   ],
   "id": "ba8089765ef04b1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[95m──────────────────────────────────────────────────\n",
      "🧠 Generation Info (BitNet GGUF)\n",
      "──────────────────────────────────────────────────\u001B[0m\n",
      "\u001B[94m💬 User Input:\u001B[0m\n",
      "<|system|>\n",
      "You are an helpful assistant.\n",
      "<|user|>\n",
      "Explain quantum mechanics in detail please.\n",
      "<|assistant|>\n",
      "Quantum mechanics is a fundamental theory in physics that describes the physical properties of nature at the scale of atoms and subatomic particles, such as electrons, protons, and photons.\n",
      "\n",
      "\u001B[92m🟢 Generated Text:\u001B[0m\n",
      " At its core, quantum mechanics is based on the idea that not all electromagnetic fields can be described by classical mechanics through classical fields\n",
      "\n",
      "\u001B[94m📊 Timings:\u001B[0m\n",
      "├─ Prefill: 2.12 ms/token, 470.98 tokens/s\n",
      "└─ Decode: 27.56 ms/token, 36.29 tokens/s\n",
      "\u001B[94m📦 Tokens:\u001B[0m\n",
      "├─ Prefilled: 64\n",
      "└─ Decoded: 25\n",
      "\u001B[94m🛑 Stop Reason:\u001B[0m Limit\n",
      "\n",
      "\u001B[95m──────────────────────────────────────────────────\n",
      "💡 Token Probabilities\n",
      "──────────────────────────────────────────────────\u001B[0m\n",
      "| Step | Token           |  Probability |\u001B[0m\n",
      "|------|-----------------|--------------|\u001B[0m\n",
      "|    1 |  At             |       61.01% |\u001B[0m\n",
      "|    2 |  its            |       47.76% |\u001B[0m\n",
      "|    3 |  core           |       79.32% |\u001B[0m\n",
      "|    4 | ,               |       99.23% |\u001B[0m\n",
      "|    5 |  quantum        |       75.94% |\u001B[0m\n",
      "|    6 |  mechanics      |       99.58% |\u001B[0m\n",
      "|    7 |  is             |       54.14% |\u001B[0m\n",
      "|    8 |  based          |       53.95% |\u001B[0m\n",
      "|    9 |  on             |       99.46% |\u001B[0m\n",
      "|   10 |  the            |       83.20% |\u001B[0m\n",
      "|   11 |  idea           |       34.69% |\u001B[0m\n",
      "|   12 |  that           |       94.93% |\u001B[0m\n",
      "|   13 |  not            |       21.65% |\u001B[0m\n",
      "|   14 |  all            |       58.40% |\u001B[0m\n",
      "|   15 |  electromagneti |       34.65% |\u001B[0m\n",
      "|   16 |  fields         |       27.16% |\u001B[0m\n",
      "|   17 |  can            |       37.28% |\u001B[0m\n",
      "|   18 |  be             |       98.78% |\u001B[0m\n",
      "|   19 |  described      |       61.49% |\u001B[0m\n",
      "|   20 |  by             |       80.42% |\u001B[0m\n",
      "|   21 |  classical      |       67.32% |\u001B[0m\n",
      "|   22 |  mechanics      |       41.99% |\u001B[0m\n",
      "|   23 |  through        |       50.49% |\u001B[0m\n",
      "|   24 |  classical      |       20.34% |\u001B[0m\n",
      "|   25 |  fields         |       16.14% |\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' At its core, quantum mechanics is based on the idea that not all electromagnetic fields can be described by classical mechanics through classical fields'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Speculative Decoding",
   "id": "775ef2800620870a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T04:12:08.762308Z",
     "start_time": "2025-10-17T04:12:02.257163Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3B-1B\n",
    "bitnet.speculative_decoding_hf(\n",
    "    large_model=AutoModelForCausalLM.from_pretrained(\n",
    "        config.model_large_path,\n",
    "        device_map=\"cpu\",\n",
    "        dtype=torch.float32\n",
    "    ),\n",
    "    text=text,\n",
    "    max_new_tokens=10,\n",
    "    num_assistant_tokens=5,\n",
    "    assistant_confidence_threshold=0.25,\n",
    "    stream=True,\n",
    "    verbose=True\n",
    ")"
   ],
   "id": "b916781114aa635a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d1ea0a952c7b407f996601afafeeda13"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are an helpful assistant.\n",
      "<|user|>\n",
      "Explain quantum mechanics in detail please.\n",
      "<|assistant|>\n",
      "Quantum mechanics is a fundamental theory in physics that describes the physical properties of nature at the scale of atoms and subatomic particles, such as electrons, protons, and "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pathfinder/miniconda3/envs/bitnet-cpp/lib/python3.9/site-packages/transformers/generation/utils.py:2532: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but got mat2 is on cuda:0, different from other tensors on cpu (when checking argument in method wrapper_CUDA_bmm)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[12], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# 3B-1B\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[43mbitnet\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mspeculative_decoding_hf\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlarge_model\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mAutoModelForCausalLM\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel_large_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcpu\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat32\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtext\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_new_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnum_assistant_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     11\u001B[0m \u001B[43m    \u001B[49m\u001B[43massistant_confidence_threshold\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.25\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     12\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m     13\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\n\u001B[1;32m     14\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/bitnet-cpp/lib/python3.9/site-packages/torch/utils/_contextlib.py:120\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    117\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    118\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    119\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 120\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/projects/BitNet/speculative_decoding.py:471\u001B[0m, in \u001B[0;36mBitNet.speculative_decoding_hf\u001B[0;34m(self, large_model, text, max_new_tokens, num_assistant_tokens, assistant_confidence_threshold, stream, verbose)\u001B[0m\n\u001B[1;32m    468\u001B[0m inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtokenizer(text, return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m, add_special_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m    469\u001B[0m input_len \u001B[38;5;241m=\u001B[39m inputs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m--> 471\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mlarge_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    472\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    473\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpad_token_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpad_token_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    474\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_new_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_new_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    475\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdo_sample\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    476\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    477\u001B[0m \u001B[43m    \u001B[49m\u001B[43massistant_model\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    478\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnum_assistant_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_assistant_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    479\u001B[0m \u001B[43m    \u001B[49m\u001B[43massistant_confidence_threshold\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43massistant_confidence_threshold\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    480\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstreamer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstreamer\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    481\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict_in_generate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    482\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_scores\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\n\u001B[1;32m    483\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    484\u001B[0m \u001B[38;5;28mprint\u001B[39m(outputs)\n\u001B[1;32m    485\u001B[0m generated \u001B[38;5;241m=\u001B[39m outputs\u001B[38;5;241m.\u001B[39msequences[\u001B[38;5;241m0\u001B[39m][input_len:]\n",
      "File \u001B[0;32m~/miniconda3/envs/bitnet-cpp/lib/python3.9/site-packages/torch/utils/_contextlib.py:120\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    117\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    118\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    119\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 120\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/bitnet-cpp/lib/python3.9/site-packages/transformers/generation/utils.py:2564\u001B[0m, in \u001B[0;36mGenerationMixin.generate\u001B[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001B[0m\n\u001B[1;32m   2561\u001B[0m model_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muse_cache\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m generation_config\u001B[38;5;241m.\u001B[39muse_cache\n\u001B[1;32m   2563\u001B[0m \u001B[38;5;66;03m# 9. Call generation mode\u001B[39;00m\n\u001B[0;32m-> 2564\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43mdecoding_method\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2565\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2566\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2567\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlogits_processor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprepared_logits_processor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2568\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstopping_criteria\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprepared_stopping_criteria\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2569\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgeneration_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2570\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mgeneration_mode_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2571\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2572\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2574\u001B[0m \u001B[38;5;66;03m# Convert to legacy cache format if requested\u001B[39;00m\n\u001B[1;32m   2575\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m   2576\u001B[0m     generation_config\u001B[38;5;241m.\u001B[39mreturn_legacy_cache \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   2577\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(result, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpast_key_values\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   2578\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(result\u001B[38;5;241m.\u001B[39mpast_key_values, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mto_legacy_cache\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2579\u001B[0m ):\n",
      "File \u001B[0;32m~/miniconda3/envs/bitnet-cpp/lib/python3.9/site-packages/transformers/generation/utils.py:3590\u001B[0m, in \u001B[0;36mGenerationMixin._assisted_decoding\u001B[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, inputs_tensor, assistant_model, assistant_tokenizer, tokenizer, **model_kwargs)\u001B[0m\n\u001B[1;32m   3586\u001B[0m     model_inputs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlogits_to_keep\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m candidate_length \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m   3588\u001B[0m \u001B[38;5;66;03m# 2.2. Run a forward pass on the candidate sequence\u001B[39;00m\n\u001B[0;32m-> 3590\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3592\u001B[0m \u001B[38;5;66;03m# 2.3. Process the new logits\u001B[39;00m\n\u001B[1;32m   3593\u001B[0m \u001B[38;5;66;03m# .float() is needed to retain precision for later logits manipulations\u001B[39;00m\n\u001B[1;32m   3594\u001B[0m new_logits \u001B[38;5;241m=\u001B[39m outputs\u001B[38;5;241m.\u001B[39mlogits[:, \u001B[38;5;241m-\u001B[39mcandidate_length \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m :]\u001B[38;5;241m.\u001B[39mto(\n\u001B[1;32m   3595\u001B[0m     dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat32, device\u001B[38;5;241m=\u001B[39minput_ids\u001B[38;5;241m.\u001B[39mdevice\n\u001B[1;32m   3596\u001B[0m )  \u001B[38;5;66;03m# excludes the input prompt if present\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/bitnet-cpp/lib/python3.9/site-packages/torch/nn/modules/module.py:1773\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1771\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1772\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1773\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/bitnet-cpp/lib/python3.9/site-packages/torch/nn/modules/module.py:1784\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1779\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1780\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1781\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1782\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1783\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1784\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1786\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1787\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/miniconda3/envs/bitnet-cpp/lib/python3.9/site-packages/transformers/utils/generic.py:918\u001B[0m, in \u001B[0;36mcan_return_tuple.<locals>.wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    916\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m return_dict_passed \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    917\u001B[0m     return_dict \u001B[38;5;241m=\u001B[39m return_dict_passed\n\u001B[0;32m--> 918\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    919\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m return_dict \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(output, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[1;32m    920\u001B[0m     output \u001B[38;5;241m=\u001B[39m output\u001B[38;5;241m.\u001B[39mto_tuple()\n",
      "File \u001B[0;32m~/miniconda3/envs/bitnet-cpp/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:459\u001B[0m, in \u001B[0;36mLlamaForCausalLM.forward\u001B[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001B[0m\n\u001B[1;32m    427\u001B[0m \u001B[38;5;129m@can_return_tuple\u001B[39m\n\u001B[1;32m    428\u001B[0m \u001B[38;5;129m@auto_docstring\u001B[39m\n\u001B[1;32m    429\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    440\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Unpack[TransformersKwargs],\n\u001B[1;32m    441\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m CausalLMOutputWithPast:\n\u001B[1;32m    442\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    443\u001B[0m \u001B[38;5;124;03m    Example:\u001B[39;00m\n\u001B[1;32m    444\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    457\u001B[0m \u001B[38;5;124;03m    \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001B[39;00m\n\u001B[1;32m    458\u001B[0m \u001B[38;5;124;03m    ```\"\"\"\u001B[39;00m\n\u001B[0;32m--> 459\u001B[0m     outputs: BaseModelOutputWithPast \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    460\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    461\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    462\u001B[0m \u001B[43m        \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    463\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    464\u001B[0m \u001B[43m        \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    465\u001B[0m \u001B[43m        \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    466\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    467\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    468\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    470\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m outputs\u001B[38;5;241m.\u001B[39mlast_hidden_state\n\u001B[1;32m    471\u001B[0m     \u001B[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/bitnet-cpp/lib/python3.9/site-packages/torch/nn/modules/module.py:1773\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1771\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1772\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1773\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/bitnet-cpp/lib/python3.9/site-packages/torch/nn/modules/module.py:1784\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1779\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1780\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1781\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1782\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1783\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1784\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1786\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1787\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/miniconda3/envs/bitnet-cpp/lib/python3.9/site-packages/transformers/utils/generic.py:1064\u001B[0m, in \u001B[0;36mcheck_model_inputs.<locals>.wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1061\u001B[0m                 monkey_patched_layers\u001B[38;5;241m.\u001B[39mappend((module, original_forward))\n\u001B[1;32m   1063\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1064\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1065\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m original_exception:\n\u001B[1;32m   1066\u001B[0m     \u001B[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001B[39;00m\n\u001B[1;32m   1067\u001B[0m     \u001B[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001B[39;00m\n\u001B[1;32m   1068\u001B[0m     \u001B[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001B[39;00m\n\u001B[1;32m   1069\u001B[0m     kwargs_without_recordable \u001B[38;5;241m=\u001B[39m {k: v \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m kwargs\u001B[38;5;241m.\u001B[39mitems() \u001B[38;5;28;01mif\u001B[39;00m k \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m recordable_keys}\n",
      "File \u001B[0;32m~/miniconda3/envs/bitnet-cpp/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:392\u001B[0m, in \u001B[0;36mLlamaModel.forward\u001B[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, cache_position, use_cache, **kwargs)\u001B[0m\n\u001B[1;32m    382\u001B[0m causal_mask \u001B[38;5;241m=\u001B[39m create_causal_mask(\n\u001B[1;32m    383\u001B[0m     config\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig,\n\u001B[1;32m    384\u001B[0m     input_embeds\u001B[38;5;241m=\u001B[39minputs_embeds,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    388\u001B[0m     position_ids\u001B[38;5;241m=\u001B[39mposition_ids,\n\u001B[1;32m    389\u001B[0m )\n\u001B[1;32m    391\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m inputs_embeds\n\u001B[0;32m--> 392\u001B[0m position_embeddings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrotary_emb\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    394\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m decoder_layer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers[: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mnum_hidden_layers]:\n\u001B[1;32m    395\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m decoder_layer(\n\u001B[1;32m    396\u001B[0m         hidden_states,\n\u001B[1;32m    397\u001B[0m         attention_mask\u001B[38;5;241m=\u001B[39mcausal_mask,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    402\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    403\u001B[0m     )\n",
      "File \u001B[0;32m~/miniconda3/envs/bitnet-cpp/lib/python3.9/site-packages/torch/nn/modules/module.py:1773\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1771\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1772\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1773\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/bitnet-cpp/lib/python3.9/site-packages/torch/nn/modules/module.py:1784\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1779\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1780\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1781\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1782\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1783\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1784\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1786\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1787\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/miniconda3/envs/bitnet-cpp/lib/python3.9/site-packages/torch/utils/_contextlib.py:120\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    117\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    118\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    119\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 120\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/bitnet-cpp/lib/python3.9/site-packages/transformers/modeling_rope_utils.py:87\u001B[0m, in \u001B[0;36mdynamic_rope_update.<locals>.wrapper\u001B[0;34m(self, x, position_ids)\u001B[0m\n\u001B[1;32m     85\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrope_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlongrope\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m     86\u001B[0m     longrope_frequency_update(\u001B[38;5;28mself\u001B[39m, position_ids, device\u001B[38;5;241m=\u001B[39mx\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m---> 87\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mrope_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/bitnet-cpp/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:101\u001B[0m, in \u001B[0;36mLlamaRotaryEmbedding.forward\u001B[0;34m(self, x, position_ids)\u001B[0m\n\u001B[1;32m     99\u001B[0m device_type \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mdevice\u001B[38;5;241m.\u001B[39mtype \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(x\u001B[38;5;241m.\u001B[39mdevice\u001B[38;5;241m.\u001B[39mtype, \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m x\u001B[38;5;241m.\u001B[39mdevice\u001B[38;5;241m.\u001B[39mtype \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmps\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    100\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautocast(device_type\u001B[38;5;241m=\u001B[39mdevice_type, enabled\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):  \u001B[38;5;66;03m# Force float32\u001B[39;00m\n\u001B[0;32m--> 101\u001B[0m     freqs \u001B[38;5;241m=\u001B[39m (\u001B[43minv_freq_expanded\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m@\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mposition_ids_expanded\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m)\n\u001B[1;32m    102\u001B[0m     emb \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat((freqs, freqs), dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m    103\u001B[0m     cos \u001B[38;5;241m=\u001B[39m emb\u001B[38;5;241m.\u001B[39mcos() \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattention_scaling\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Expected all tensors to be on the same device, but got mat2 is on cuda:0, different from other tensors on cpu (when checking argument in method wrapper_CUDA_bmm)"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 32bit-1bit\n",
    "bitnet.speculative_decoding(\n",
    "    text=text,\n",
    "    max_new_tokens=100,\n",
    "    num_assistant_tokens=5,\n",
    "    confidence_threshold=0.25,\n",
    "    verbose=True\n",
    ")"
   ],
   "id": "5912a38dc912fd7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Evaluation",
   "id": "8718388d62233e0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## HellaSwag",
   "id": "4c291ae1fef0f1ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "fcb638c4f837c9f3",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
