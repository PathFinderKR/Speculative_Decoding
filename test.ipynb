{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "import requests\n",
    "import json\n",
    "from typing import List, Dict, Optional\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer"
   ],
   "id": "fdc0c7ae9596bcf7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def speculative_decoding_hf(\n",
    "        self,\n",
    "        large_model,\n",
    "        text: str,\n",
    "        max_new_tokens: int = 128,\n",
    "        num_assistant_tokens: int = 4,\n",
    "        confidence_threshold: float = 0.4,\n",
    "        seed: int = 42,\n",
    "        verbose: bool = False,\n",
    "):\n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"\\033[95m\" + \"‚îÄ\" * 50 + \"\\033[0m\")\n",
    "        print(\"‚ú® Speculative Decoding\")\n",
    "        print(f\"‚îú‚îÄ Target Model: {large_model.config.name_or_path}\")\n",
    "        print(f\"‚îú‚îÄ Draft Model: {self.model.config.name_or_path}\")\n",
    "        print(f\"‚îî‚îÄ Draft Length: {num_assistant_tokens}, Confidence: {confidence_threshold:.1f}\")\n",
    "        print(\"\\033[95m\" + \"‚îÄ\" * 50 + \"\\033[0m\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    step = 1\n",
    "    total_draft_tokens = 0\n",
    "    total_accepted_draft_tokens = 0\n",
    "\n",
    "    # Initial prompt\n",
    "    prompt_tokens = self.tokenizer(\n",
    "        text, return_tensors=\"pt\", add_special_tokens=False\n",
    "    ).input_ids.to(large_model.device)\n",
    "    generated_token_ids = []\n",
    "    past_key_values = None\n",
    "\n",
    "    # Generation loop\n",
    "    while len(generated_token_ids) < max_new_tokens:\n",
    "        current_input_ids = torch.cat(\n",
    "            [\n",
    "                prompt_tokens,\n",
    "                torch.tensor([generated_token_ids], dtype=torch.long, device=large_model.device)\n",
    "            ],\n",
    "            dim=-1,\n",
    "        )\n",
    "\n",
    "        # 1. Draft Generation\n",
    "        draft_outputs = self.model.generate(\n",
    "            input_ids=current_input_ids.to(self.model.device),\n",
    "            max_new_tokens=num_assistant_tokens,\n",
    "            do_sample=False,\n",
    "            seed=seed,\n",
    "            use_cache=True\n",
    "        )\n",
    "        draft_ids = draft_outputs[0, current_input_ids.shape[-1]:].to(large_model.device)\n",
    "\n",
    "        if len(draft_ids) == 0:\n",
    "            if verbose: print(\"‚ö†Ô∏è Draft model produced no new tokens. Stopping.\")\n",
    "            break\n",
    "        total_draft_tokens += len(draft_ids)\n",
    "\n",
    "        # 2. Target Í≤ÄÏ¶ù (large_model ÏÇ¨Ïö©)\n",
    "        verification_ids = torch.cat([current_input_ids, draft_ids.unsqueeze(0)], dim=-1)\n",
    "        outputs = large_model(\n",
    "            input_ids=verification_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            use_cache=True\n",
    "        )\n",
    "        logits = outputs.logits\n",
    "        past_key_values = outputs.past_key_values\n",
    "\n",
    "        if verbose:\n",
    "            current_text = self.tokenizer.decode(current_input_ids[0])\n",
    "            print(\"\\n\" + \"\\033[95m\" + \"-\" * 10 + f\"Step {step}\" + \"-\" * 10 + \"\\033[0m\")\n",
    "            print(f\"\\033[94mDraft Input:\\033[0m\\n{current_text}\")\n",
    "            print(f\"\\033[94mDraft Output\\033[0m\\n{self.tokenizer.decode(draft_ids.tolist(), skip_special_tokens=False)}\")\n",
    "            print(f\"‚îå{'‚îÄ'*5}‚î¨{'‚îÄ'*17}‚î¨{'‚îÄ'*14}‚î¨{'‚îÄ'*20}‚î¨{'‚îÄ'*17}‚îê\")\n",
    "            print(f\"‚îÇ {'Idx':<3s} ‚îÇ {'Draft Token':<15s} ‚îÇ {'Target Prob':>12s} ‚îÇ {'Status':<18s} ‚îÇ {'Corrected':<15s} ‚îÇ\")\n",
    "            print(f\"‚îú{'‚îÄ'*5}‚îº{'‚îÄ'*17}‚îº{'‚îÄ'*14}‚îº{'‚îÄ'*20}‚îº{'‚îÄ'*17}‚î§\")\n",
    "\n",
    "        accepted_count = 0\n",
    "        for i in range(len(draft_ids)):\n",
    "            # 1. Probabilities\n",
    "            target_logit = logits[:, current_input_ids.shape[-1] + i - 1, :]\n",
    "            probs = torch.softmax(target_logit, dim=-1)\n",
    "\n",
    "            draft_token_id = draft_ids[i]\n",
    "            draft_token_prob = probs[0, draft_token_id].item()\n",
    "            draft_token_str = self.tokenizer.decode(draft_token_id).replace('\\n', '\\\\n')\n",
    "\n",
    "            # 2. Accept/Reject\n",
    "            if draft_token_prob >= confidence_threshold:\n",
    "                accepted_count += 1\n",
    "                if verbose:\n",
    "                    status = \"\\033[92m‚úÖ Accepted\\033[0m\"\n",
    "                    corrected_str = \"-\"\n",
    "                    print(f\"‚îÇ {i + 1:<3d} ‚îÇ {draft_token_str:<15.15s} ‚îÇ {draft_token_prob:>12.2%} ‚îÇ {status:<26s} ‚îÇ {corrected_str:<15s} ‚îÇ\")\n",
    "\n",
    "            else:\n",
    "                corrected_token = torch.argmax(target_logit, dim=-1).item()\n",
    "                if verbose:\n",
    "                    corrected_str = self.tokenizer.decode(corrected_token).replace('\\n', '\\\\n')\n",
    "                    status = \"\\033[91m‚ùå Rejected\\033[0m\"\n",
    "                    print(f\"‚îÇ {i + 1:<3d} ‚îÇ {draft_token_str:<15.15s} ‚îÇ {draft_token_prob:>12.2%} ‚îÇ {status:<26s} ‚îÇ {corrected_str:<15.15s} ‚îÇ\")\n",
    "                    print(f\"‚îî{'‚îÄ' * 5}‚î¥{'‚îÄ' * 17}‚î¥{'‚îÄ' * 14}‚î¥{'‚îÄ' * 20}‚î¥{'‚îÄ' * 17}‚îò\")\n",
    "                if accepted_count > 0:\n",
    "                    generated_token_ids.extend(draft_ids[:accepted_count].tolist())\n",
    "                generated_token_ids.append(corrected_token)\n",
    "                total_accepted_draft_tokens += accepted_count\n",
    "                break\n",
    "        else: # All draft tokens accepted\n",
    "            total_accepted_draft_tokens += accepted_count\n",
    "            generated_token_ids.extend(draft_ids.tolist())\n",
    "            last_logit = logits[:, -1, :]\n",
    "            next_token = torch.argmax(last_logit, dim=-1).item()\n",
    "            generated_token_ids.append(next_token)\n",
    "            if verbose:\n",
    "                print(f\"‚îî{'‚îÄ' * 5}‚î¥{'‚îÄ' * 17}‚î¥{'‚îÄ' * 14}‚î¥{'‚îÄ' * 20}‚î¥{'‚îÄ' * 17}‚îò\")\n",
    "                accepted_token_strs = [self.tokenizer.decode(t).replace('\\n', '\\\\n') for t in draft_ids.tolist()]\n",
    "                next_token_str = self.tokenizer.decode(next_token).replace('\\n', '\\\\n')\n",
    "                print(f\"‚úÖ \\033[92mAccepted all {accepted_count} tokens: \\033[0m{accepted_token_strs}\")\n",
    "                print(f\"‚úÖ \\033[92mGenerated Target Token: \\033[0m{next_token_str}\")\n",
    "\n",
    "        if self.tokenizer.eos_token_id in generated_token_ids:\n",
    "            eos_index = generated_token_ids.index(self.tokenizer.eos_token_id)\n",
    "            generated_token_ids = generated_token_ids[:eos_index]\n",
    "            if verbose: print(\"üõë EOS token generated. Stopping.\")\n",
    "            break\n",
    "        step += 1\n",
    "\n",
    "    end_time = time.time()\n",
    "    final_text = self.tokenizer.decode(generated_token_ids, skip_special_tokens=False)\n",
    "\n",
    "    if verbose:\n",
    "        total_time = end_time - start_time\n",
    "        num_generated = len(generated_token_ids)\n",
    "        acceptance_rate = (total_accepted_draft_tokens / total_draft_tokens) * 100 if total_draft_tokens > 0 else 0\n",
    "        latency = (total_time / num_generated) * 1000 if num_generated > 0 else float('inf')\n",
    "        throughput = num_generated / total_time if total_time > 0 else float('inf')\n",
    "\n",
    "        print(\"\\n\" + \"\\033[95m\" + \"‚îÄ\" * 50 + \"\\033[0m\")\n",
    "        print(\"üèÅ Speculative Decoding Finished\")\n",
    "        print(f\"\\033[94müí¨ User Input:\\033[0m\\n{text}\")\n",
    "        print(f\"\\n\\033[92müü¢ Generated Text:\\033[0m\\n{final_text}\")\n",
    "        print(\"\\n\\033[94müìä Performance:\\033[0m\")\n",
    "        print(f\"‚îú‚îÄ Total Time: {total_time:.2f}s\")\n",
    "        print(f\"‚îú‚îÄ Latency: {latency:.2f} ms/token\")\n",
    "        print(f\"‚îî‚îÄ  Throughput: {throughput:.2f} tokens/s\")\n",
    "        print(f\"\\033[94m‚ú® Speculative Stats:\\033[0m\")\n",
    "        print(f\"‚îú‚îÄ Acceptance Rate: {acceptance_rate:.2f}%\")\n",
    "        print(f\"‚îú‚îÄ Total Drafted Tokens: {total_draft_tokens}\")\n",
    "        print(f\"‚îî‚îÄ Total Accepted Draft Tokens: {total_accepted_draft_tokens}\")\n",
    "\n",
    "    return final_text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
